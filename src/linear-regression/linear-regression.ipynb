{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a417c6c",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf23bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = load_diabetes()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "column_names = data['feature_names']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbffc78",
   "metadata": {},
   "source": [
    "# OLS via Augmentation & Inversion\n",
    "\n",
    "Given a dataset with:\n",
    "\n",
    "- Feature matrix $X \\in \\mathbb{R}^{N \\times p}$\n",
    "- Target vector $y \\in \\mathbb{R}^N$\n",
    "\n",
    "We fit a linear model with weights $W$ and bias $b$:  \n",
    "$$\n",
    "\\hat{y} = X\\,W + b\\,\\mathbf{I}\n",
    "$$\n",
    "\n",
    "## 1. Augmented Design Matrix\n",
    "\n",
    "We augment \\(X\\) by adding a column of ones:  \n",
    "$$\n",
    "X_{\\text{aug}} = \\bigl[\\,X \\;\\big|\\; \\mathbf{1}\\bigr] \\tag{1}\n",
    "$$\n",
    "\n",
    "and define the parameter vector:  \n",
    "$$\n",
    "\\beta = \\begin{pmatrix} W \\\\[4pt] b \\end{pmatrix} \\tag{2}\n",
    "$$\n",
    "\n",
    "The model can now be written as:  \n",
    "$$\n",
    "\\hat{y} = X_{\\text{aug}}\\,\\beta \\tag{3}\n",
    "$$\n",
    "\n",
    "## 2. Least Squares Objective\n",
    "\n",
    "The OLS loss is  \n",
    "$$\n",
    "L(\\beta) = \\|\\,y - X_{\\text{aug}}\\,\\beta\\|_2^2 \\tag{4}\n",
    "$$\n",
    "\n",
    "Expanding:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\beta)\n",
    "&= (y - X_{\\text{aug}}\\beta)^\\top (y - X_{\\text{aug}}\\beta) \\\\[6pt]\n",
    "&= y^\\top y \\;-\\; 2\\,\\beta^\\top X_{\\text{aug}}^\\top y \\;+\\; \\beta^\\top X_{\\text{aug}}^\\top X_{\\text{aug}}\\,\\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## 3. Solve for $\\beta$\n",
    "\n",
    "Set the gradient to zero:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\beta}\n",
    "&= -2\\,X_{\\text{aug}}^\\top (y - X_{\\text{aug}}\\beta) \\\\[4pt]\n",
    "&= -2\\,X_{\\text{aug}}^\\top y \\;+\\; 2\\,X_{\\text{aug}}^\\top X_{\\text{aug}}\\,\\beta\n",
    "\\;=\\;0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Rearrange to get the normal equation:  \n",
    "$$\n",
    "X_{\\text{aug}}^\\top X_{\\text{aug}}\\,\\beta \\;=\\; X_{\\text{aug}}^\\top y\n",
    "$$\n",
    "\n",
    "Hence the closed-form solution is  \n",
    "$$\n",
    "\\boxed{\n",
    "\\beta \n",
    "= \\bigl(X_{\\text{aug}}^\\top X_{\\text{aug}}\\bigr)^{-1}\\,X_{\\text{aug}}^\\top\\,y\n",
    "} \\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44798c14",
   "metadata": {},
   "source": [
    "# Solving with Matrix Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eedfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 53.85, R^2: 0.45\n"
     ]
    }
   ],
   "source": [
    "# 1. Augment training features with a column of ones\n",
    "X_aug = np.hstack([X_train, np.ones((len(X_train), 1))]) # X_aug = [X_train, 1] # Add a column of ones for the bias term\n",
    "\n",
    "# 2. Apply the normal equation\n",
    "beta = np.linalg.inv(X_aug.T @ X_aug) @ (X_aug.T @ y_train) # (X_aug^T X_aug)^{-1} X_aug^T y_train\n",
    "\n",
    "# 3. Unpack weights and bias\n",
    "W, b = beta[:-1], beta[-1] # W = beta[:-1] and b = beta[-1] # Last element is the bias term\n",
    "\n",
    "y_pred = X_test @ W + b # Y = X@W + b\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = 1 - (np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
    "print(f\"RMSE: {rmse:.2f}, R^2: {r2:.2f}\")\n",
    "# Get weights and bias from matrix inversion solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe941a",
   "metadata": {},
   "source": [
    "# Solving with sklearn LinearRegression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8079211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 53.85, R^2: 0.45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(f\"RMSE: {rmse:.2f}, R^2: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97d88f",
   "metadata": {},
   "source": [
    "## PCA via Eigen-Decomposition of the Covariance (U) Matrix\n",
    "\n",
    "1. **Center** your data:  \n",
    "   $$X_c = X - \\operatorname{mean}(X,\\;{\\rm axis}=0)$$  \n",
    "2. **Form** the covariance‐style matrix:  \n",
    "   $$U = \\frac{1}{N-1}\\,X_c^\\top X_c$$  \n",
    "3. **Eigendecompose** \\(U\\):  \n",
    "   $$U\\,v_i = \\lambda_i\\,v_i$$  \n",
    "4. **Compare** $\\{\\lambda_i,\\,v_i\\}$ to `sklearn.decomposition.PCA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1de007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues (manual):    [0.009  0.0033 0.0028 0.0023 0.0015 0.0014 0.0012 0.001  0.0002 0.    ]\n",
      "Explained var (sklearn): [0.009  0.0033 0.0028 0.0023 0.0015 0.0014 0.0012 0.001  0.0002 0.    ]\n",
      "PC1 alignment: 1.0\n",
      "PC2 alignment: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- assume X_train is your (N, p) feature matrix ---\n",
    "N, p = X_train.shape\n",
    "\n",
    "# 1. Center\n",
    "Xc = X_train - X_train.mean(axis=0)\n",
    "\n",
    "# 2. Covariance-style matrix\n",
    "U = Xc.T @ Xc / (N - 1)\n",
    "\n",
    "# 3. Eigendecomposition\n",
    "eigvals, eigvecs = np.linalg.eig(U)\n",
    "idx = np.argsort(eigvals)[::-1] # Sort eigenvalues and eigenvectors in descending order\n",
    "eigvals, eigvecs = eigvals[idx], eigvecs[:, idx]\n",
    "\n",
    "# 4. sklearn PCA\n",
    "pca = PCA().fit(X_train)\n",
    "\n",
    "# Print and compare\n",
    "print(\"Eigenvalues (manual):   \", np.round(eigvals, 4))\n",
    "print(\"Explained var (sklearn):\", np.round(pca.explained_variance_, 4))\n",
    "\n",
    "# Compare first two principal directions (up to sign)\n",
    "for i in range(2):\n",
    "    manual_pc = eigvecs[:, i]\n",
    "    skl_pc    = pca.components_[i]\n",
    "    corr = np.dot(manual_pc, skl_pc)\n",
    "    print(f\"PC{i+1} alignment:\", np.sign(corr)*np.round(np.abs(corr),4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b8eb1",
   "metadata": {},
   "source": [
    "# Additional Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0a924",
   "metadata": {},
   "source": [
    "## Eigen‐Decomposition of $X_{\\text{aug}}^T X_{\\text{aug}}$ and Inversion\n",
    "\n",
    "Given the augmented design matrix  \n",
    "$$\n",
    "X_{\\text{aug}} \\in \\mathbb{R}^{N\\times (p+1)},\n",
    "$$\n",
    "the Gram matrix  \n",
    "$$\n",
    "U = X_{\\text{aug}}^T X_{\\text{aug}}\n",
    "$$\n",
    "is square of shape $(p+1)\\times(p+1)$.  We can diagonalize it as follows:\n",
    "\n",
    "1. **Eigendecomposition**  \n",
    "   $$\n",
    "   U = V \\, D \\, V^T,\n",
    "   $$\n",
    "   where  \n",
    "   - $D = \\mathrm{diag}(d_1, \\dots, d_{p+1})$  \n",
    "   - $V^T V = I$  \n",
    "\n",
    "2. **Inverse via eigenvalues**  \n",
    "   $$\n",
    "   U^{-1} = V \\, D^{-1} \\, V^T,\n",
    "   $$\n",
    "   with  \n",
    "   $$\n",
    "   D^{-1} = \\mathrm{diag}\\bigl(\\tfrac1{d_1}, \\dots, \\tfrac1{d_{p+1}}\\bigr).\n",
    "   $$\n",
    "\n",
    "3. **Solve for** $\\beta$  \n",
    "   Plugging into the normal equation\n",
    "   $$\n",
    "   \\beta = U^{-1}\\,X_{\\text{aug}}^T\\,y\n",
    "   $$\n",
    "   gives the closed-form\n",
    "   $$\n",
    "   \\boxed{\n",
    "     \\beta = V\\,D^{-1}\\,V^T\\,X_{\\text{aug}}^T\\,y\n",
    "   }\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9579a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Inversion RMSE: 53.85, R^2: 0.45\n"
     ]
    }
   ],
   "source": [
    "U = X_aug.T @ X_aug \n",
    "eigvals, eigvecs = np.linalg.eig(U)\n",
    "idx = np.argsort(eigvals)[::-1]  # Sort eigenvalues and eigenvectors in descending order\n",
    "eigvals, eigvecs = eigvals[idx], eigvecs[:, idx]\n",
    "\n",
    "D = np.diag(eigvals)\n",
    "V = eigvecs\n",
    "VT = eigvecs.T\n",
    "inv_D = np.diag(1 / eigvals) \n",
    "inv_U = V @ inv_D @ V.T \n",
    "beta_matrix_inv = inv_U @ X_aug.T @ y_train\n",
    "W_matrix_inv, b_matrix_inv = beta_matrix_inv[:-1], beta_matrix_inv[-1]  # Last element is the bias term\n",
    "\n",
    "y_pred_matrix_inv = X_test @ W_matrix_inv + b_matrix_inv  # Y = X@W + b\n",
    "mse_matrix_inv = np.mean((y_test - y_pred_matrix_inv) ** 2)\n",
    "rmse_matrix_inv = np.sqrt(mse_matrix_inv)\n",
    "r2_matrix_inv = 1 - (np.sum((y_test - y_pred_matrix_inv) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
    "print(f\"Matrix Inversion RMSE: {rmse_matrix_inv:.2f}, R^2: {r2_matrix_inv:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392bbfc",
   "metadata": {},
   "source": [
    "## Ridge Regression via Augmentation & Eigen‐Inversion\n",
    "\n",
    "We start with the augmented design matrix  \n",
    "$$\n",
    "X_{\\text{aug}} = \\left[\\, X \\;\\big|\\; \\mathbf{1} \\,\\right] \\in \\mathbb{R}^{N \\times (p+1)}\n",
    "$$\n",
    "and solve the regularized least‐squares problem\n",
    "$$\n",
    "\\underset{\\beta}{\\min}\\;\\|\\,y - X_{\\rm aug}\\beta\\|_2^2 \\;+\\;\\lambda\\|\\beta\\|_2^2.\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "### 1. Normal equation for Ridge\n",
    "\n",
    "Taking the gradient and setting to zero gives\n",
    "$$\n",
    "X_{\\rm aug}^T X_{\\rm aug}\\,\\beta + \\lambda\\,\\beta \n",
    "= X_{\\rm aug}^T y\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\boxed{\n",
    "  (X_{\\rm aug}^T X_{\\rm aug} + \\lambda I)\\,\\beta\n",
    "  = X_{\\rm aug}^T y\n",
    "}\n",
    "\\tag{2}\n",
    "$$\n",
    "so the closed‐form is\n",
    "$$\n",
    "\\beta_{\\rm ridge}\n",
    "= \\bigl(X_{\\rm aug}^T X_{\\rm aug} + \\lambda I\\bigr)^{-1}\\,X_{\\rm aug}^T y.\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "### 2. Eigen-decomposition of $X_{\\text{aug}}^\\top X_{\\text{aug}}$\n",
    "\n",
    "\n",
    "Let\n",
    "$$\n",
    "U = X_{\\rm aug}^T X_{\\rm aug}\n",
    "= V\\,D\\,V^T,\n",
    "\\quad\n",
    "D = \\mathrm{diag}(d_1,\\dots,d_{p+1}),\n",
    "\\quad\n",
    "V^TV = I.\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "U + \\lambda I\n",
    "= V\\,(D + \\lambda I)\\,V^T,\n",
    "\\quad\n",
    "\\bigl(U + \\lambda I\\bigr)^{-1}\n",
    "= V\\,(D + \\lambda I)^{-1}\\,V^T.\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "### 3. Ridge solution via eigen‐inversion\n",
    "\n",
    "Substitute into (3):\n",
    "$$\n",
    "\\boxed{\n",
    "  \\beta_{\\rm ridge}\n",
    "  = \\bigl(U + \\lambda I\\bigr)^{-1}\\,X_{\\rm aug}^T y\n",
    "  = V\\,(D + \\lambda I)^{-1}\\,V^T\\,X_{\\rm aug}^T y\n",
    "}\n",
    "\\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b462be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression (eigen-inversion) → RMSE: 53.69,  R²: 0.46\n"
     ]
    }
   ],
   "source": [
    "U = X_aug.T @ X_aug                                  # shape = (p+1, p+1)\n",
    "\n",
    "# 3. Eigendecompose U\n",
    "eigvals, eigvecs = np.linalg.eig(U)                  # eigvals.shape = (p+1,), eigvecs.shape = (p+1, p+1)\n",
    "idx = np.argsort(eigvals)[::-1]                      # indices for sorting descending\n",
    "eigvals = eigvals[idx]\n",
    "eigvecs = eigvecs[:, idx]\n",
    "\n",
    "# 4. Form the inverse of (D + λI) in the eigenbasis\n",
    "lambda_ridge = 0.01\n",
    "inv_D_plus = np.diag(1.0 / (eigvals + lambda_ridge)) # shape = (p+1, p+1)\n",
    "\n",
    "# 5. Reconstruct (U + λI)^{-1} = V (D+λI)^{-1} V^T\n",
    "V = eigvecs\n",
    "inv_U_ridge = V @ inv_D_plus @ V.T                   # shape = (p+1, p+1)\n",
    "\n",
    "# 6. Solve for beta_ridge = (U + λI)^{-1} X_aug^T y_train\n",
    "beta_ridge = inv_U_ridge @ X_aug.T @ y_train         # shape = (p+1,)\n",
    "\n",
    "# 7. Unpack weights and bias\n",
    "W_ridge = beta_ridge[:-1]                            # shape = (p,)\n",
    "b_ridge = beta_ridge[-1]                             # scalar\n",
    "\n",
    "# 8. Predict on the test set\n",
    "y_pred = X_test @ W_ridge + b_ridge                   # shape = (n_test,)\n",
    "\n",
    "# 9. Evaluate performance\n",
    "mse  = np.mean((y_test - y_pred)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "r2   = 1 - np.sum((y_test - y_pred)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
    "\n",
    "print(f\"Ridge Regression (eigen-inversion) → RMSE: {rmse:.2f},  R²: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650908b",
   "metadata": {},
   "source": [
    "# sklearn ridge regression for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d15386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression (sklearn) → RMSE: 53.69,  R²: 0.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_model = Ridge(alpha=0.01)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_sklearn = ridge_model.predict(X_test)\n",
    "mse_sklearn = np.mean((y_test - y_pred_sklearn) ** 2)\n",
    "rmse_sklearn = np.sqrt(mse_sklearn)\n",
    "r2_sklearn = ridge_model.score(X_test, y_test)\n",
    "print(f\"Ridge Regression (sklearn) → RMSE: {rmse_sklearn:.2f},  R²: {r2_sklearn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753eca66",
   "metadata": {},
   "source": [
    "### Commentary: Ridge Regression and Eigenvalue Adjustment\n",
    "\n",
    "In ridge regression, the only change from ordinary least squares (OLS) happens **at the level of eigenvalue inversion**.\n",
    "\n",
    "In OLS, the closed-form solution involves:\n",
    "$$\n",
    "\\beta_{\\text{OLS}} = V\\,D^{-1}\\,V^\\top\\,X_{\\text{aug}}^\\top y\n",
    "$$\n",
    "where:\n",
    "- $V$ contains the eigenvectors of $X_{\\text{aug}}^\\top X_{\\text{aug}}$\n",
    "- $D = \\mathrm{diag}(d_1, d_2, \\dots, d_{p+1})$ contains its eigenvalues\n",
    "- $D^{-1} = \\mathrm{diag}\\left(\\frac{1}{d_1}, \\frac{1}{d_2}, \\dots, \\frac{1}{d_{p+1}}\\right)$\n",
    "\n",
    "---\n",
    "\n",
    "In **ridge regression**, we modify this inversion step:\n",
    "$$\n",
    "\\beta_{\\text{ridge}} = V\\,(D + \\lambda I)^{-1}\\,V^\\top\\,X_{\\text{aug}}^\\top y\n",
    "$$\n",
    "This means that instead of dividing by $d_i$, we divide by $d_i + \\lambda$.  \n",
    "So the shrinkage factor becomes:\n",
    "$$\n",
    "\\frac{1}{d_i + \\lambda}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Why this improves stability\n",
    "\n",
    "- If an eigenvalue $d_i$ is **very small** (i.e. a nearly flat direction), then $1/d_i$ becomes large → unstable OLS.\n",
    "- Ridge fixes this by **adding** $\\lambda$, making all eigenvalues invertible and bounded away from zero.\n",
    "- This stabilizes the solution, especially when $X^T X$ is ill-conditioned or near-singular.\n",
    "\n",
    "---\n",
    "\n",
    "### When ridge has little effect\n",
    "\n",
    "If the **smallest eigenvalue** \\(d_{\\min}\\) is already large, then:\n",
    "$$\n",
    "\\frac{1}{d_{\\min} + \\lambda} \\approx \\frac{1}{d_{\\min}}\n",
    "$$\n",
    "So ridge shrinkage becomes negligible, and the solution is essentially the same as OLS:\n",
    "$$\n",
    "\\lim_{\\lambda \\to 0} \\beta_{\\text{ridge}} = \\beta_{\\text{OLS}}\n",
    "$$\n",
    "\n",
    "In such cases, **regularization is not needed**, because the original matrix is already well-conditioned.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
